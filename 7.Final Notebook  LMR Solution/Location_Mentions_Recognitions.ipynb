{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyYcdz5qX5oh"
      },
      "source": [
        "### All paths used in this notebook are Relative, thus the folder structure should not be altered\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmCS8jAuPw_v"
      },
      "outputs": [],
      "source": [
        "#required libraries , uncomment to install\n",
        "\n",
        "! pip install pandas --q\n",
        "! pip install numpy  --q\n",
        "! pip install spacy  --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Zh1Txq9x4K",
        "outputId": "383aa11f-61cb-4dfc-80d5-f4db42725ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount the drive for uploading the folder with model and input file.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNNigxPIaRtW"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/model-best-updated'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvY_K3toAbLB"
      },
      "outputs": [],
      "source": [
        "#import the required libraries\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import spacy #framework used for building the ML model\n",
        "import os \n",
        "from spacy import displacy\n",
        "import json\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "from spacy.tokens import DocBin\n",
        "import warnings\n",
        "from spacy import displacy\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\",None)\n",
        "pd.set_option(\"display.max_rows\",None)\n",
        "pd.set_option(\"display.max_colwidth\",2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smTsu1MwEDBy"
      },
      "outputs": [],
      "source": [
        "# model is loaded from the folder model-best-updated\n",
        "\n",
        "model=spacy.load(model_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOvLzkLfFXwh"
      },
      "outputs": [],
      "source": [
        "#The input file example should be like below:\n",
        "\"\"\"\n",
        "\n",
        "{\n",
        "    \"tweet_id\":\"TWEET ID\",\n",
        "    \"text\":\"YOUR TWEET TEXT GOES HERE\"\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# specify the path for the input file (JSONL)\n",
        "\n",
        "upload_path=\"/content/train.jsonl\" #path for the input file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M8_l53Zbhbd"
      },
      "outputs": [],
      "source": [
        "def open_json(path):\n",
        "  mode='r'\n",
        "  dict_list = []\n",
        "  with open(path, mode) as data:\n",
        "    for jobject in data:\n",
        "      jdict = json.loads(jobject)\n",
        "      dict_list.append(jdict)\n",
        "\n",
        "  return dict_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USV52B3pIgGl"
      },
      "outputs": [],
      "source": [
        "#function to compute the start and end offset of the location mention indentified\n",
        "def find_index(sentence,word):\n",
        "    str2=word\n",
        "    str1=sentence\n",
        "    start_index=str1.index(str2)\n",
        "    length_word=len(str2)\n",
        "    end_index=start_index+length_word\n",
        "    start=start_index\n",
        "    end=end_index\n",
        "    return start,end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfCXdzC4dT1_"
      },
      "outputs": [],
      "source": [
        "#store the data tweet and tweet id in the variable\n",
        "data_c = open_json(upload_path)\n",
        "\n",
        "data = []\n",
        "tweet_id = []\n",
        "\n",
        "for d in data_c:\n",
        "  data.append(d['text'])\n",
        "  tweet_id.append(d['tweet_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45i0Uriif8Ym"
      },
      "outputs": [],
      "source": [
        "# compute the output results\n",
        "# data_located_list=\"\"\n",
        "twitter_id=tweet_id\n",
        "\n",
        "all_locations = []\n",
        "\n",
        "for data_item in data:\n",
        "  test_preds=model(data_item) #find recognition\n",
        "  list_locations = []\n",
        "  for entity in test_preds.ents:\n",
        "    if entity.label_=='Location':\n",
        "      list_locations.append(entity.text)\n",
        "  all_locations.append(list_locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzjPbmm1n5Sy"
      },
      "outputs": [],
      "source": [
        "DICT_LIST = []\n",
        "for a, locs in zip(data, all_locations):\n",
        "  dict_list = []\n",
        "  for b in locs:\n",
        "    data_located_Dict={}\n",
        "    data_located_Dict['text']=b\n",
        "    start_index, end_index = find_index(a, b) \n",
        "    data_located_Dict['start_offset'] = start_index\n",
        "    data_located_Dict['end_offset'] = end_index\n",
        "    dict_list.append(data_located_Dict)\n",
        "  DICT_LIST.append(dict_list)\n",
        "  \n",
        "  \n",
        "                    \n",
        "#Final Results\n",
        "Model_Results = []\n",
        "for tid, dicts in zip(tweet_id, DICT_LIST):\n",
        "  Model_Results.append({\"tweet_id\":tid,\"location_mentions\":dicts})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCOENhgfwGZw"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for item in Model_Results:\n",
        "  results.append(item.__str__() +'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7Xpt6aBzwTW"
      },
      "outputs": [],
      "source": [
        "# print the results\n",
        "\n",
        "with open(\"output.jsonl\", \"a\") as f:\n",
        "   f.writelines(results)\n",
        "   f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JfTJCAHUScq"
      },
      "outputs": [],
      "source": [
        "#The other codes for Data loading,data preparation,cleaning and modelling are available in the docker images\n",
        "# use this command :   docker pull 19991806/ingenuity_lmr_geoai"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
